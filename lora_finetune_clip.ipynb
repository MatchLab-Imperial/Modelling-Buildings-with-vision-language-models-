{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5f2010",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip /content/normal_roof.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426ba251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Pretrained model name (you can pick another variant)\n",
    "MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "# Load the processor\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 1) Load the base CLIP model\n",
    "base_model = CLIPModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                     # LoRA rank\n",
    "    lora_alpha=32,            # LoRA alpha\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Consider specifying exact paths if needed, e.g.\n",
    "    # \"vision_model.encoder.layers.0.self_attn.q_proj\", etc.\n",
    "    # You could also include projections if you plan to adapt heads:\n",
    "    # [\"vision_model.visual_projection\", \"text_model.text_projection\", ...]\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    # For CLIP fine-tuning for contrastive features, FEATURE_EXTRACTION is fine.\n",
    "    # If adding a classification head, consider SEQ_CLS, etc.\n",
    "    task_type=TaskType.FEATURE_EXTRACTION\n",
    ")\n",
    "\n",
    "# 3) Apply LoRA to the model\n",
    "lora_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Print trainable parameter counts and ratio\n",
    "lora_model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0951c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchvision import transforms\n",
    "\n",
    "class MaterialClassificationDataset(Dataset):\n",
    "    def __init__(self, image_dir, class_names, processor, image_transforms=None):\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.class_names = class_names\n",
    "        self.class_to_idx = {name: i for i, name in enumerate(class_names)}\n",
    "        self.idx_to_class = {i: name for i, name in enumerate(class_names)}\n",
    "        self.processor = processor\n",
    "        self.image_transforms = image_transforms\n",
    "\n",
    "        for class_name in class_names:\n",
    "            class_dir = os.path.join(image_dir, class_name)\n",
    "            if not os.path.isdir(class_dir):\n",
    "                print(f\"Warning: directory {class_dir} does not exist.\")\n",
    "                continue\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "                    self.image_paths.append(os.path.join(class_dir, img_name))\n",
    "                    self.labels.append(self.class_to_idx[class_name])\n",
    "\n",
    "        # Text prompts (for image–text contrastive models like CLIP)\n",
    "        self.text_prompts = [f\"a photo of {name}\" for name in self.class_names]\n",
    "        self.text_inputs = self.processor(\n",
    "            text=self.text_prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: failed to read image {img_path}. {e}\")\n",
    "            return None\n",
    "\n",
    "        if self.image_transforms:\n",
    "            image = self.image_transforms(image)\n",
    "\n",
    "        image_inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": image_inputs.pixel_values.squeeze(0),\n",
    "            \"input_ids\": self.text_inputs.input_ids,\n",
    "            \"attention_mask\": self.text_inputs.attention_mask,\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# === Image size helper ===\n",
    "def resolve_image_size(processor):\n",
    "    size = processor.feature_extractor.size\n",
    "    if isinstance(size, dict):\n",
    "        # Use height/width if present (e.g., {\"height\": 224, \"width\": 224})\n",
    "        h = size.get(\"height\") or size.get(\"shortest_edge\") or 224\n",
    "        w = size.get(\"width\") or size.get(\"shortest_edge\") or 224\n",
    "        return (h, w)\n",
    "    elif isinstance(size, int):\n",
    "        return (size, size)\n",
    "    elif isinstance(size, (list, tuple)) and len(size) == 2:\n",
    "        return tuple(size)\n",
    "    else:\n",
    "        return (224, 224)  # fallback\n",
    "\n",
    "# Get the model’s preferred input size\n",
    "image_size = resolve_image_size(processor)\n",
    "\n",
    "# === Data augmentation / preprocessing ===\n",
    "# train_transforms = transforms.Compose([\n",
    "#     transforms.RandomResizedCrop(image_size),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "# ])\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(image_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),  # convert PIL image to Tensor in [0, 1]\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet mean\n",
    "                         std=[0.229, 0.224, 0.225])   # ImageNet std\n",
    "])\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.CenterCrop(image_size),\n",
    "])\n",
    "\n",
    "# === Example usage ===\n",
    "class_names = [\"flat\", \"gabled\", \"gambrel\", \"hipped\", \"mansard\", \"pyramidal\", \"saltbox\", \"skillion\"]\n",
    "train_image_dir = \"/content/sorted_images\"\n",
    "val_image_dir = \"/content/sorted_images\"\n",
    "\n",
    "train_dataset = MaterialClassificationDataset(train_image_dir, class_names, processor, image_transforms=train_transforms)\n",
    "val_dataset = MaterialClassificationDataset(val_image_dir, class_names, processor, image_transforms=val_transforms)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Drop None samples that failed to load\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    if not batch:\n",
    "        return None\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76486eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lora_model.to(device)\n",
    "\n",
    "# Optimizer (PEFT ensures only LoRA parameters are updated)\n",
    "optimizer = optim.AdamW(lora_model.parameters(), lr=1e-4)  # You may tune LR; 1e-4 or 5e-5 are common starting points\n",
    "num_epochs = 20  # Adjust based on dataset size and convergence; with only dozens of images, more epochs may be needed (watch for overfitting)\n",
    "\n",
    "# Assume train_dataloader and class_names are defined\n",
    "# class_names = train_dataset.class_names\n",
    "# text_inputs_global = processor(text=[f\"a photo of {name}\" for name in class_names],\n",
    "#                               return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    lora_model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Training]\")\n",
    "    for batch in progress_bar:\n",
    "        if batch is None:\n",
    "            continue  # Skip empty batches (if collate_fn returns None)\n",
    "\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "\n",
    "        # Text inputs are usually identical for all samples in a batch (one prompt per class).\n",
    "        # In our Dataset, text_inputs is (num_classes, seq_len), shared across items.\n",
    "        # DataLoader stacks them to (batch_size, num_classes, seq_len); we only need one copy.\n",
    "        # So we take the first element along the batch dimension.\n",
    "        text_input_ids_all_classes = batch[\"input_ids\"][0].to(device)       # (num_classes, seq_len)\n",
    "        text_attention_mask_all_classes = batch[\"attention_mask\"][0].to(device)  # (num_classes, seq_len)\n",
    "\n",
    "        # Get image features\n",
    "        image_features = lora_model.get_image_features(pixel_values=pixel_values)  # (batch_size, embed_dim)\n",
    "\n",
    "        # Get text features (for all classes)\n",
    "        text_features = lora_model.get_text_features(\n",
    "            input_ids=text_input_ids_all_classes,\n",
    "            attention_mask=text_attention_mask_all_classes\n",
    "        )  # (num_classes, embed_dim)\n",
    "\n",
    "        # Feature normalization (standard in CLIP)\n",
    "        image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "        # Similarity logits\n",
    "        # logit_scale is a learnable temperature parameter in CLIP\n",
    "        logit_scale = lora_model.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_features @ text_features.t()  # (batch_size, num_classes)\n",
    "\n",
    "        labels = batch[\"label\"].to(device)  # (batch_size,)\n",
    "\n",
    "        loss = F.cross_entropy(logits_per_image, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1} - Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # --- Validation (optional but strongly recommended) ---\n",
    "    if 'val_dataloader' in locals() and val_dataloader is not None:\n",
    "        lora_model.eval()\n",
    "        total_val_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        val_progress_bar = tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Validation]\")\n",
    "        with torch.no_grad():\n",
    "            for batch in val_progress_bar:\n",
    "                if batch is None:\n",
    "                    continue\n",
    "\n",
    "                pixel_values = batch[\"pixel_values\"].to(device)\n",
    "                text_input_ids_all_classes = batch[\"input_ids\"][0].to(device)\n",
    "                text_attention_mask_all_classes = batch[\"attention_mask\"][0].to(device)\n",
    "                labels = batch[\"label\"].to(device)\n",
    "\n",
    "                image_features = lora_model.get_image_features(pixel_values=pixel_values)\n",
    "                text_features = lora_model.get_text_features(\n",
    "                    input_ids=text_input_ids_all_classes,\n",
    "                    attention_mask=text_attention_mask_all_classes\n",
    "                )\n",
    "\n",
    "                image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "                text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "                logit_scale = lora_model.logit_scale.exp()\n",
    "                logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "\n",
    "                loss = F.cross_entropy(logits_per_image, labels)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                preds = torch.argmax(logits_per_image, dim=1)\n",
    "                correct_predictions += (preds == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "                val_progress_bar.set_postfix({\"val_loss\": loss.item()})\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        accuracy = correct_predictions / total_samples if total_samples > 0 else 0.0\n",
    "        print(f\"Epoch {epoch+1} - Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save LoRA weights\n",
    "save_path = \"my_lora_clip_material_classifier\"\n",
    "lora_model.save_pretrained(save_path)\n",
    "\n",
    "# Also save the processor and class names for future loading\n",
    "import json\n",
    "with open(os.path.join(save_path, \"class_names.json\"), \"w\") as f:\n",
    "    json.dump(class_names, f)  # assumes class_names is in scope\n",
    "processor.save_pretrained(save_path)\n",
    "\n",
    "print(f\"LoRA model saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01e6031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from PIL import Image\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "# --- Load model and related assets ---\n",
    "base_model_name = \"openai/clip-vit-base-patch32\"   # Base CLIP model\n",
    "lora_adapter_path = \"my_lora_clip_material_classifier\"  # Path to LoRA adapter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1) Load the base CLIP model\n",
    "base_model_for_inference = CLIPModel.from_pretrained(base_model_name)\n",
    "\n",
    "# 2) Load and attach the LoRA adapter to the base model\n",
    "inference_model = PeftModel.from_pretrained(base_model_for_inference, lora_adapter_path)\n",
    "inference_model.to(device)\n",
    "inference_model.eval()\n",
    "\n",
    "# 3) Load the processor (image & text pre/post-processing)\n",
    "processor_for_inference = CLIPProcessor.from_pretrained(lora_adapter_path)\n",
    "\n",
    "# 4) Load class names\n",
    "with open(os.path.join(lora_adapter_path, \"class_names.json\"), \"r\") as f:\n",
    "    class_names_for_inference = json.load(f)\n",
    "\n",
    "# 5) Prepare text prompts for all classes and compute text features once\n",
    "text_prompts_inference = [f\"a photo of {name}\" for name in class_names_for_inference]\n",
    "text_inputs_inference = processor_for_inference(\n",
    "    text=text_prompts_inference,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_features_inference = inference_model.get_text_features(\n",
    "        input_ids=text_inputs_inference.input_ids,\n",
    "        attention_mask=text_inputs_inference.attention_mask\n",
    "    )\n",
    "    # L2-normalize features (standard CLIP practice)\n",
    "    text_features_inference = text_features_inference / text_features_inference.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "def classify_single_image(image_path, model, processor, text_features_all_classes, class_names_list):\n",
    "    \"\"\"\n",
    "    Classify a single image using CLIP+LoRA by comparing image features to precomputed text features.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "        model (nn.Module): CLIP model with LoRA adapter applied.\n",
    "        processor (CLIPProcessor): Preprocessing utility.\n",
    "        text_features_all_classes (Tensor): (num_classes, embed_dim) normalized features.\n",
    "        class_names_list (List[str]): Class names aligned with text features.\n",
    "\n",
    "    Returns:\n",
    "        (predicted_class, confidence) if success, or (error_message, 0.0) if failed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        return f\"Failed to read image: {e}\", 0.0\n",
    "\n",
    "    # Preprocess image\n",
    "    image_inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Encode image and normalize\n",
    "        image_features = model.get_image_features(pixel_values=image_inputs.pixel_values)\n",
    "        image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "        # Compute similarity logits and softmax to get class probabilities\n",
    "        logit_scale = model.logit_scale.exp()\n",
    "        logits = logit_scale * image_features @ text_features_all_classes.t()\n",
    "        probabilities = logits.softmax(dim=-1).squeeze()  # (num_classes,)\n",
    "\n",
    "        predicted_idx = torch.argmax(probabilities).item()\n",
    "        predicted_class = class_names_list[predicted_idx]\n",
    "        confidence = probabilities[predicted_idx].item()\n",
    "\n",
    "    return predicted_class, confidence\n",
    "\n",
    "\n",
    "# --- Inference example ---\n",
    "test_image_path = \"/content/17062472.png\"\n",
    "predicted_material, confidence = classify_single_image(\n",
    "    test_image_path,\n",
    "    inference_model,\n",
    "    processor_for_inference,\n",
    "    text_features_inference,\n",
    "    class_names_for_inference\n",
    ")\n",
    "print(f\"Image: {test_image_path}\")\n",
    "print(f\"Predicted class: {predicted_material}, Confidence: {confidence:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cameractrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
