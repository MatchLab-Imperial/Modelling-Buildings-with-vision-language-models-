{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a16b642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define image transforms\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class BuildingDataset(Dataset):\n",
    "    def __init__(self, json_path, transform=None):\n",
    "        \"\"\"\n",
    "        Read JSON data, parse building metadata, and provide DataLoader access.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.data = json.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        - metadata: Metadata from the JSON record (including latitude/longitude, ground elevation, straight-line distance, etc.)\n",
    "        - image: PyTorch Tensor (after transforms)\n",
    "        \"\"\"\n",
    "        metadata = self.data[idx]\n",
    "        image_root = r\"\"\n",
    "        image_path = os.path.join(image_root, metadata.get(\"fixed_pitch_image\", \"\"))\n",
    "\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                assert callable(self.transform), f\"Transform is not callable: {type(self.transform)}\"\n",
    "                image = self.transform(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            image = torch.zeros(3, 224, 224)  # Placeholder black image to prevent crashes\n",
    "\n",
    "        return metadata, image\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Process batched data for DataLoader:\n",
    "    - Keep metadata as list[dict]\n",
    "    - Stack images into a Tensor\n",
    "    \"\"\"\n",
    "    metadata_batch = [item[0] for item in batch]\n",
    "    images_batch = torch.stack([item[1] for item in batch])\n",
    "    return metadata_batch, images_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9035b76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" load data \"\"\"\n",
    "from PIL import Image\n",
    "thing_category_names = [\"building\",\"car\", \"person\", \"bus\", \"traffic light\",'bridge','statue','funtain','bench','billboard','Roadblock','street lamp']\n",
    "stuff_category_names_building = [\n",
    "    \"brick\", \"plaster\", \"concrete\", \"metal\", \"stone\", \"wood\", \"glass\", \"sandstone\", \"metal_sheet\"]\n",
    "stuff_category_names = [\"street road\",\"wood\",\"sky\", \"trees\", \"sidewalk\",'sand','water','grass']\n",
    "category_names = thing_category_names + stuff_category_names_building + stuff_category_names\n",
    "category_name_to_id = {\n",
    "    category_name: i for i, category_name in enumerate(category_names)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae35a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" import \"\"\"\n",
    "import os, sys\n",
    "sys.path.append('F:/Personal_projects_MSC/panoptic-segment-anything/Grounded-Segment-Anything')\n",
    "import random\n",
    "import requests\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from scipy import ndimage\n",
    "from PIL import Image\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from segments import SegmentsClient\n",
    "from segments.export import colorize\n",
    "from segments.utils import bitmap2file\n",
    "from getpass import getpass\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import math\n",
    "# Grounding DINO\n",
    "import GroundingDINO.groundingdino.datasets.transforms as T\n",
    "from GroundingDINO.groundingdino.models import build_model\n",
    "from GroundingDINO.groundingdino.util import box_ops\n",
    "from GroundingDINO.groundingdino.util.slconfig import SLConfig\n",
    "from GroundingDINO.groundingdino.util.utils import clean_state_dict\n",
    "from GroundingDINO.groundingdino.util.inference import annotate, predict\n",
    "\n",
    "# segment anything\n",
    "from segment_anything import build_sam, SamPredictor\n",
    "\n",
    "# CLIPSeg\n",
    "from transformers import CLIPSegProcessor, CLIPSegForImageSegmentation\n",
    "\"\"\" model import \"\"\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "if device != \"cpu\":\n",
    "    try:\n",
    "        from GroundingDINO.groundingdino import _C\n",
    "    except:\n",
    "        warnings.warn(\n",
    "            \"Failed to load custom C++ ops. Running on CPU mode Only in groundingdino!\"\n",
    "        )\n",
    "        \n",
    "def load_model_hf(repo_id, filename, ckpt_config_filename, device, local_files_only=False):\n",
    "\n",
    "    if local_files_only:\n",
    "        cache_config_file = os.path.join(repo_id, ckpt_config_filename)\n",
    "        cache_file = os.path.join(repo_id, filename)\n",
    "\n",
    "        if not os.path.exists(cache_config_file):\n",
    "            raise FileNotFoundError(f\"Config file not found: {cache_config_file}\")\n",
    "        if not os.path.exists(cache_file):\n",
    "            raise FileNotFoundError(f\"Model file not found: {cache_file}\")\n",
    "    else:\n",
    "        cache_config_file = hf_hub_download(repo_id=repo_id, filename=ckpt_config_filename)\n",
    "        cache_file = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "\n",
    "    args = SLConfig.fromfile(cache_config_file)\n",
    "    model = build_model(args)\n",
    "    args.device = device\n",
    "\n",
    "    checkpoint = torch.load(cache_file, map_location=\"cpu\")\n",
    "    log = model.load_state_dict(clean_state_dict(checkpoint[\"model\"]), strict=False)\n",
    "    print(\"Model loaded from {} \\n => {}\".format(cache_file, log))\n",
    "\n",
    "    _ = model.eval()\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "# Use this command for evaluate the Grounding DINO model\n",
    "# Or you can download the model by yourself\n",
    "ckpt_repo_id = \"ckpt/ShilongLiu/GroundingDINO\"\n",
    "ckpt_filename = \"groundingdino_swinb_cogcoor.pth\"\n",
    "ckpt_config_filename = \"GroundingDINO_SwinB.cfg.py\"\n",
    "\n",
    "groundingdino_model = load_model_hf(\n",
    "    ckpt_repo_id, ckpt_filename, ckpt_config_filename, device, local_files_only=True \n",
    ")\n",
    "\n",
    "\"\"\" load SAM \"\"\"\n",
    "sam_checkpoint = \"ckpt/sam/sam_vit_h_4b8939.pth\"\n",
    "sam = build_sam(checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "sam_predictor = SamPredictor(sam)\n",
    "\n",
    "\"\"\" load clipseg \"\"\"\n",
    "clipseg_processor = CLIPSegProcessor.from_pretrained(\"ckpt/CIDAS/clipseg-rd64-refined\")\n",
    "clipseg_model = CLIPSegForImageSegmentation.from_pretrained(\n",
    "    \"ckpt/CIDAS/clipseg-rd64-refined\"\n",
    ")\n",
    "clipseg_model.to(device)\n",
    "clipseg_processor\n",
    "clipseg_processor.feature_extractor.size = {\"height\": 224, \"width\": 224}\n",
    "\n",
    "\"\"\" help funcs \"\"\"\n",
    "def download_image(url):\n",
    "    return Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "\n",
    "def load_image_for_dino(image):\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.RandomResize([800], max_size=1333),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    dino_image, _ = transform(image, None)\n",
    "    return dino_image\n",
    "\n",
    "\n",
    "def dino_detection(\n",
    "    model,\n",
    "    image,\n",
    "    image_array,\n",
    "    category_names,\n",
    "    category_name_to_id,\n",
    "    box_threshold,\n",
    "    text_threshold,\n",
    "    device,\n",
    "    visualize=False,\n",
    "):\n",
    "    detection_prompt = \" . \".join(category_names)\n",
    "    dino_image = load_image_for_dino(image)\n",
    "    dino_image = dino_image.to(device)\n",
    "    with torch.no_grad():\n",
    "        boxes, logits, phrases = predict(\n",
    "            model=model,\n",
    "            image=dino_image,\n",
    "            caption=detection_prompt,\n",
    "            box_threshold=box_threshold,\n",
    "            text_threshold=text_threshold,\n",
    "            device=device,\n",
    "            # remove_combined=True,\n",
    "        )\n",
    "    category_ids = [category_name_to_id[phrase] for phrase in phrases]\n",
    "\n",
    "    if visualize:\n",
    "        annotated_frame = annotate(\n",
    "            image_source=image_array, boxes=boxes, logits=logits, phrases=phrases\n",
    "        )\n",
    "        annotated_frame = annotated_frame[..., ::-1]  # BGR to RGB\n",
    "        visualization = Image.fromarray(annotated_frame)\n",
    "        return boxes, category_ids, visualization\n",
    "    else:\n",
    "        return boxes, category_ids, phrases\n",
    "\n",
    "\n",
    "def sam_masks_from_dino_boxes(predictor, image_array, boxes, device):\n",
    "    # box: normalized box xywh -> unnormalized xyxy\n",
    "    H, W, _ = image_array.shape\n",
    "    boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
    "    transformed_boxes = predictor.transform.apply_boxes_torch(\n",
    "        boxes_xyxy, image_array.shape[:2]\n",
    "    ).to(device)\n",
    "    thing_masks, _, _ = predictor.predict_torch(\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        boxes=transformed_boxes,\n",
    "        multimask_output=False,\n",
    "    )\n",
    "    return thing_masks\n",
    "\n",
    "\n",
    "def preds_to_semantic_inds(preds, threshold):\n",
    "    flat_preds = preds.reshape((preds.shape[0], -1))\n",
    "    # Initialize a dummy \"unlabeled\" mask with the threshold\n",
    "    flat_preds_with_treshold = torch.full(\n",
    "        (preds.shape[0] + 1, flat_preds.shape[-1]), threshold\n",
    "    )\n",
    "    flat_preds_with_treshold[1 : preds.shape[0] + 1, :] = flat_preds\n",
    "\n",
    "    # Get the top mask index for each pixel\n",
    "    semantic_inds = torch.topk(flat_preds_with_treshold, 1, dim=0).indices.reshape(\n",
    "        (preds.shape[-2], preds.shape[-1])\n",
    "    )\n",
    "\n",
    "    return semantic_inds\n",
    "\n",
    "\n",
    "def clipseg_segmentation(\n",
    "    processor, model, image, category_names, background_threshold, device\n",
    "):\n",
    "    inputs = processor(\n",
    "        text=category_names,\n",
    "        images=[image] * len(category_names),\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    if len(logits.shape) == 2:\n",
    "      logits = logits.unsqueeze(0)\n",
    "    # resize the outputs\n",
    "    upscaled_logits = nn.functional.interpolate(\n",
    "        logits.unsqueeze(1),\n",
    "        size=(image.size[1], image.size[0]),\n",
    "        mode=\"bilinear\",\n",
    "    )\n",
    "    preds = torch.sigmoid(upscaled_logits.squeeze(dim=1))\n",
    "    semantic_inds = preds_to_semantic_inds(preds, background_threshold)\n",
    "    return preds, semantic_inds\n",
    "\n",
    "\n",
    "def semantic_inds_to_shrunken_bool_masks(\n",
    "    semantic_inds, shrink_kernel_size, num_categories\n",
    "):\n",
    "    shrink_kernel = np.ones((shrink_kernel_size, shrink_kernel_size))\n",
    "\n",
    "    bool_masks = torch.zeros((num_categories, *semantic_inds.shape), dtype=bool)\n",
    "    for category in range(num_categories):\n",
    "        binary_mask = semantic_inds == category\n",
    "        shrunken_binary_mask_array = (\n",
    "            ndimage.binary_erosion(binary_mask.numpy(), structure=shrink_kernel)\n",
    "            if shrink_kernel_size > 0\n",
    "            else binary_mask.numpy()\n",
    "        )\n",
    "        bool_masks[category] = torch.from_numpy(shrunken_binary_mask_array)\n",
    "\n",
    "    return bool_masks\n",
    "\n",
    "\n",
    "def clip_and_shrink_preds(semantic_inds, preds, shrink_kernel_size, num_categories):\n",
    "    # convert semantic_inds to shrunken bool masks\n",
    "    bool_masks = semantic_inds_to_shrunken_bool_masks(\n",
    "        semantic_inds, shrink_kernel_size, num_categories\n",
    "    ).to(preds.device)\n",
    "\n",
    "    sizes = [\n",
    "        torch.sum(bool_masks[i].int()).item() for i in range(1, bool_masks.size(0))\n",
    "    ]\n",
    "    max_size = max(sizes)\n",
    "    relative_sizes = [size / max_size for size in sizes] if max_size > 0 else sizes\n",
    "\n",
    "    # use bool masks to clip preds\n",
    "    clipped_preds = torch.zeros_like(preds)\n",
    "    for i in range(1, bool_masks.size(0)):\n",
    "        float_mask = bool_masks[i].float()\n",
    "        clipped_preds[i - 1] = preds[i - 1] * float_mask\n",
    "\n",
    "    return clipped_preds, relative_sizes\n",
    "\n",
    "\n",
    "def sample_points_based_on_preds(preds, N):\n",
    "    height, width = preds.shape\n",
    "    weights = preds.ravel()\n",
    "    indices = np.arange(height * width)\n",
    "\n",
    "    # Randomly sample N indices based on the weights\n",
    "    sampled_indices = random.choices(indices, weights=weights, k=N)\n",
    "\n",
    "    # Convert the sampled indices into (col, row) coordinates\n",
    "    sampled_points = [(index % width, index // width) for index in sampled_indices]\n",
    "\n",
    "    return sampled_points\n",
    "\n",
    "\n",
    "def upsample_pred(pred, image_source):\n",
    "    pred = pred.unsqueeze(dim=0)\n",
    "    original_height = image_source.shape[0]\n",
    "    original_width = image_source.shape[1]\n",
    "\n",
    "    larger_dim = max(original_height, original_width)\n",
    "    aspect_ratio = original_height / original_width\n",
    "\n",
    "    # upsample the tensor to the larger dimension\n",
    "    # upsampled_tensor = F.interpolate(\n",
    "    #     pred, size=(larger_dim, larger_dim), mode=\"bilinear\", align_corners=False\n",
    "    # )\n",
    "    upsampled_tensor = F.interpolate(pred, size=(larger_dim, larger_dim), mode=\"bilinear\", align_corners=False)\n",
    "    # remove the padding (at the end) to get the original image resolution\n",
    "    if original_height > original_width:\n",
    "        target_width = int(upsampled_tensor.shape[3] * aspect_ratio)\n",
    "        upsampled_tensor = upsampled_tensor[:, :, :, :target_width]\n",
    "    else:\n",
    "        target_height = int(upsampled_tensor.shape[2] * aspect_ratio)\n",
    "        upsampled_tensor = upsampled_tensor[:, :, :target_height, :]\n",
    "    return upsampled_tensor.squeeze(dim=1)\n",
    "\n",
    "\n",
    "def sam_mask_from_points(predictor, image_array, points):\n",
    "    points_array = np.array(points)\n",
    "    # we only sample positive points, so labels are all 1\n",
    "    points_labels = np.ones(len(points))\n",
    "    # we don't use predict_torch here cause it didn't seem to work...\n",
    "    masks, scores, logits = predictor.predict(\n",
    "        point_coords=points_array,\n",
    "        point_labels=points_labels,\n",
    "    )\n",
    "    # max over the 3 segmentation levels\n",
    "    total_pred = torch.max(torch.sigmoid(torch.tensor(logits)), dim=0)[0].unsqueeze(\n",
    "        dim=0\n",
    "    )\n",
    "    # logits are 256x256 -> upsample back to image shape\n",
    "    upsampled_pred = upsample_pred(total_pred, image_array)\n",
    "    return upsampled_pred\n",
    "\n",
    "def get_masked_building_image(building_masks, image):\n",
    "    \"\"\"\n",
    "    Apply masks to the original image, leaving masked regions visible and making the rest black.\n",
    "\n",
    "    Args:\n",
    "    - building_masks: List of masks (torch tensors) with shape [1, H, W]\n",
    "    - image: Original image array with shape (H, W, 3)\n",
    "\n",
    "    Returns:\n",
    "    - masked_image: Image with applied masks, where unmasked regions are black\n",
    "    \"\"\"\n",
    "    # Convert image to RGB format (ensure it is 3 channels)\n",
    "    masked_image_array = np.array(image).copy()\n",
    "\n",
    "    # Initialize a combined mask with the same height and width as the image\n",
    "    combined_mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.bool_)\n",
    "\n",
    "    # Combine all masks into one\n",
    "    for building_mask in building_masks:\n",
    "        building_mask_np = building_mask.cpu().numpy().squeeze()  # Shape (H, W)\n",
    "        if building_mask_np.shape != image.shape[:2]:\n",
    "            raise ValueError(f\"Mask shape {building_mask_np.shape} does not match image shape {image.shape[:2]}\")\n",
    "        combined_mask |= building_mask_np  # Combine masks using logical OR\n",
    "\n",
    "    # Set unmasked regions to black\n",
    "    masked_image_array[~combined_mask] = 0\n",
    "\n",
    "    return Image.fromarray(masked_image_array)\n",
    "\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30 / 255, 144 / 255, 255 / 255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "\n",
    "def show_segmentation_preds(preds, category_names):\n",
    "    len_cats = len(category_names)\n",
    "    _, ax = plt.subplots(1, len_cats + 1, figsize=(3 * (len_cats + 1), 4))\n",
    "    [a.axis(\"off\") for a in ax.flatten()]\n",
    "    ax[0].imshow(image)\n",
    "    [ax[i + 1].imshow(preds[i].cpu()) for i in range(len_cats)]\n",
    "    [\n",
    "        ax[i + 1].text(0, -15, category_name)\n",
    "        for i, category_name in enumerate(category_names)\n",
    "    ]\n",
    "\n",
    "\n",
    "\"\"\" pipeline \"\"\"\n",
    "\n",
    "def generate_buiding_panoptic_mask(\n",
    "    image,\n",
    "    thing_category_names,\n",
    "    stuff_category_names_building,\n",
    "    stuff_category_names,\n",
    "    category_name_to_id,\n",
    "    dino_model,\n",
    "    sam_predictor,\n",
    "    clipseg_processor,\n",
    "    clipseg_model,\n",
    "    device,\n",
    "    dino_box_threshold=0.3,\n",
    "    dino_text_threshold=0.25,\n",
    "    segmentation_background_threshold=0.1,\n",
    "    shrink_kernel_size=20,\n",
    "    num_samples_factor=1000,\n",
    "):\n",
    "    image = image.convert(\"RGB\")\n",
    "    image_array = np.asarray(image)\n",
    "\n",
    "    # compute SAM image embedding\n",
    "    sam_predictor.set_image(image_array)\n",
    "\n",
    "    # detect boxes for \"thing\" categories using Grounding DINO\n",
    "    thing_category_ids = []\n",
    "    thing_masks = []\n",
    "    thing_boxes = []\n",
    "    if len(thing_category_names) > 0:\n",
    "        thing_boxes, thing_category_ids, _ = dino_detection(\n",
    "            dino_model,\n",
    "            image,\n",
    "            image_array,\n",
    "            thing_category_names,\n",
    "            category_name_to_id,\n",
    "            dino_box_threshold,\n",
    "            dino_text_threshold,\n",
    "            device,\n",
    "        )\n",
    "        if len(thing_boxes) > 0:\n",
    "            # get segmentation masks for the thing boxes\n",
    "            thing_masks = sam_masks_from_dino_boxes(\n",
    "                sam_predictor, image_array, thing_boxes, device\n",
    "            )\n",
    "    \n",
    "    # get building masks\n",
    "    sam_predictor.set_image(image_array)\n",
    "    thing_masks = sam_masks_from_dino_boxes(sam_predictor, image_array, thing_boxes, device)\n",
    "\n",
    "    building_masks = [mask for idx, mask in enumerate(thing_masks) if thing_category_ids[idx] == 0]\n",
    "\n",
    "    masked_image = get_masked_building_image(building_masks, image_array)\n",
    "\n",
    "    if len(stuff_category_names_building) >0:\n",
    "        clipseg_preds_building, clipseg_semantic_inds_building = clipseg_segmentation(\n",
    "            clipseg_processor,\n",
    "            clipseg_model,\n",
    "            masked_image,\n",
    "            stuff_category_names_building,\n",
    "            segmentation_background_threshold,\n",
    "            device,\n",
    "        )\n",
    "        clipsed_clipped_preds_building, relative_sizes_building = clip_and_shrink_preds(\n",
    "            clipseg_semantic_inds_building,\n",
    "            clipseg_preds_building,\n",
    "            shrink_kernel_size,\n",
    "            len(stuff_category_names_building) + 1,\n",
    "        )       \n",
    "        sam_preds = torch.zeros_like(clipsed_clipped_preds_building)\n",
    "        for i in range(clipsed_clipped_preds_building.shape[0]):\n",
    "            clipseg_preds_building = clipsed_clipped_preds_building[i]\n",
    "            # for each \"stuff\" category, sample points in the rough segmentation mask\n",
    "            num_samples = int(relative_sizes_building[i] * num_samples_factor)\n",
    "            if num_samples == 0:\n",
    "                continue\n",
    "            points = sample_points_based_on_preds(\n",
    "                clipseg_preds_building.cpu().numpy(), num_samples\n",
    "            )\n",
    "            if len(points) == 0:\n",
    "                continue\n",
    "            # use SAM to get mask for points\n",
    "            pred = sam_mask_from_points(sam_predictor, image_array, points)\n",
    "            sam_preds[i] = pred\n",
    "        sam_semantic_inds_building = preds_to_semantic_inds(\n",
    "            sam_preds, segmentation_background_threshold\n",
    "        )\n",
    "\n",
    "\n",
    "    if len(stuff_category_names) > 0:\n",
    "        # get rough segmentation masks for \"stuff\" categories using CLIPSeg\n",
    "        clipseg_preds, clipseg_semantic_inds = clipseg_segmentation(\n",
    "            clipseg_processor,\n",
    "            clipseg_model,\n",
    "            image,\n",
    "            stuff_category_names,\n",
    "            segmentation_background_threshold,\n",
    "            device,\n",
    "        )\n",
    "        # remove things from stuff masks\n",
    "        clipseg_semantic_inds_without_things = clipseg_semantic_inds.clone()\n",
    "        if len(thing_boxes) > 0:\n",
    "            combined_things_mask = torch.any(thing_masks, dim=0)\n",
    "            clipseg_semantic_inds_without_things[combined_things_mask[0]] = 0\n",
    "        # clip CLIPSeg preds based on non-overlapping semantic segmentation inds (+ optionally shrink the mask of each category)\n",
    "        # also returns the relative size of each category\n",
    "        clipsed_clipped_preds, relative_sizes = clip_and_shrink_preds(\n",
    "            clipseg_semantic_inds_without_things,\n",
    "            clipseg_preds,\n",
    "            shrink_kernel_size,\n",
    "            len(stuff_category_names) + 1,\n",
    "        )\n",
    "        # get finer segmentation masks for the \"stuff\" categories using SAM\n",
    "        sam_preds = torch.zeros_like(clipsed_clipped_preds)\n",
    "        for i in range(clipsed_clipped_preds.shape[0]):\n",
    "            clipseg_pred = clipsed_clipped_preds[i]\n",
    "            # for each \"stuff\" category, sample points in the rough segmentation mask\n",
    "            num_samples = int(relative_sizes[i] * num_samples_factor)\n",
    "            if num_samples == 0:\n",
    "                continue\n",
    "            points = sample_points_based_on_preds(\n",
    "                clipseg_pred.cpu().numpy(), num_samples\n",
    "            )\n",
    "            if len(points) == 0:\n",
    "                continue\n",
    "            # use SAM to get mask for points\n",
    "            pred = sam_mask_from_points(sam_predictor, image_array, points)\n",
    "            sam_preds[i] = pred\n",
    "        sam_semantic_inds = preds_to_semantic_inds(\n",
    "            sam_preds, segmentation_background_threshold\n",
    "        )\n",
    "\n",
    "    # combine the thing inds and the stuff inds into panoptic inds\n",
    "    panoptic_inds = (\n",
    "        sam_semantic_inds.clone()\n",
    "        if len(stuff_category_names) > 0\n",
    "        else torch.zeros(image_array.shape[0], image_array.shape[1], dtype=torch.long)\n",
    "    )\n",
    "\n",
    "    panoptic_inds_buildings = (\n",
    "        sam_semantic_inds_building.clone()\n",
    "        if len(stuff_category_names_building) > 0\n",
    "        else torch.zeros(image_array.shape[0], image_array.shape[1], dtype=torch.long)\n",
    "    )\n",
    "\n",
    "    combined_mask = torch.zeros_like(panoptic_inds, dtype=torch.bool, device=panoptic_inds.device)\n",
    "    for building_mask in building_masks:\n",
    "        building_mask = building_mask.to(panoptic_inds.device)  # Ensure the mask is on the same device\n",
    "        combined_mask |= building_mask.squeeze(0).bool()  # Ensure the mask is 2D and boolean\n",
    "\n",
    "    # Update panoptic_inds to remove building mask regions\n",
    "    panoptic_inds[combined_mask] = 0\n",
    "\n",
    "    # Update panoptic_inds_buildings to retain only building mask regions\n",
    "    panoptic_inds_buildings[~combined_mask] = 0\n",
    "    panoptic_inds_buildings[panoptic_inds_buildings > 0] += len(stuff_category_names)\n",
    "    panoptic_inds[panoptic_inds_buildings > 0]=panoptic_inds_buildings[panoptic_inds_buildings > 0]\n",
    "\n",
    "\n",
    "    ind = len(stuff_category_names) + 1 + len(stuff_category_names_building)\n",
    "    for thing_mask in thing_masks:\n",
    "        thing_mask = thing_mask.squeeze(dim=0).bool()  # Ensure thing_mask is 2D, boolean, and on the same device\n",
    "        valid_indices = thing_mask & (panoptic_inds_buildings.to(device) == 0)  # Both conditions satisfied\n",
    "        panoptic_inds[valid_indices] = ind\n",
    "        ind += 1\n",
    "\n",
    "    return panoptic_inds, thing_category_ids, panoptic_inds_buildings, building_masks\n",
    "\n",
    "\"\"\" mask visualization \"\"\"\n",
    "def display_images_in_grid(image, images, l ,rows, cols, path,figsize=(25, 20)):\n",
    "    \"\"\"\n",
    "    Display images in a grid layout.\n",
    "\n",
    "    Parameters:\n",
    "    - images: list of image arrays (e.g., from plt.imread or PIL.Image)\n",
    "    - rows: number of rows in the grid\n",
    "    - cols: number of columns in the grid\n",
    "    - figsize: size of the figure\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=figsize)\n",
    "    axs = axs.flatten()  # Flatten the 2D array of axes for easy iteration\n",
    "    for i, ax in enumerate(axs):\n",
    "        if i < l:\n",
    "            ax.imshow(image)\n",
    "            ax.imshow(colorize(images==i+1), alpha=0.5)\n",
    "            ax.axis('off')  # Turn off axis for clarity\n",
    "            # print(i)\n",
    "            ax.set_title(list(category_name_to_id.keys())[list(category_name_to_id.values()).index(thing_category_ids[i])])\n",
    "        else:\n",
    "            ax.axis('off')  # Hide empty plots if images < rows*cols\n",
    "    plt.savefig(path)\n",
    "    # plt.tight_layout()  # Reduce padding between subplots\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "# display_images_in_grid(image,panoptic_inds, len(thing_category_ids), rows=5, cols=6)\n",
    "\"\"\" mask visualization \"\"\"\n",
    "\n",
    "def pixel_to_camera_coordinates(u, v, D, K):\n",
    "    \"\"\"\n",
    "    Convert pixel coordinates (u, v) and depth D to 3D coordinates in the camera coordinate system.\n",
    "\n",
    "    Args:\n",
    "        u, v: Pixel coordinates (x, y).\n",
    "        D: Depth value (in meters).\n",
    "        K: Camera intrinsic matrix.\n",
    "\n",
    "    Returns:\n",
    "        X_camera, Y_camera, Z_camera: 3D coordinates in the camera coordinate system.\n",
    "    \"\"\"\n",
    "    fx, fy = K[0, 0], K[1, 1]  # Focal lengths\n",
    "    cx, cy = K[0, 2], K[1, 2]  # Principal points\n",
    "\n",
    "    X_camera = (u - cx) * D / fx\n",
    "    Y_camera = (v - cy) * D / fy\n",
    "    Z_camera = D\n",
    "\n",
    "    return X_camera, Y_camera, Z_camera\n",
    "\n",
    "\n",
    "\n",
    "def extract_top_and_bottom_points(segmentation_mask, plot=False):\n",
    "    \"\"\"\n",
    "    Extract the highest and corresponding lowest points from the segmentation mask.\n",
    "\n",
    "    Args:\n",
    "        segmentation_mask: Binary segmentation mask (target area = 1, background = 0).\n",
    "        plot: Whether to visualize the points (default: False).\n",
    "\n",
    "    Returns:\n",
    "        (u_top, v_top), (u_bottom, v_bottom): Pixel coordinates of the top and bottom points of the target.\n",
    "    \"\"\"\n",
    "    indices = np.where(segmentation_mask == 1)\n",
    "    v_coords = indices[0]  # y-coordinates\n",
    "    u_coords = indices[1]  # x-coordinates\n",
    "\n",
    "    # 1. Find the highest point (minimum v)\n",
    "    top_idx = np.argmin(v_coords)\n",
    "    u_top, v_top = u_coords[top_idx], v_coords[top_idx]\n",
    "\n",
    "    # 2. Find the lowest point with the same x-coordinate as the top point (maximum v)\n",
    "    same_x_indices = np.where(u_coords == u_top)  # Get all indices where u == u_top\n",
    "    v_bottom = np.max(v_coords[same_x_indices])  # Find the maximum v in those points\n",
    "    u_bottom = u_top  # x-coordinate remains the same\n",
    "\n",
    "    if plot:\n",
    "        plt.imshow(segmentation_mask, cmap=\"gray\")\n",
    "        plt.plot(u_top, v_top, 'ro', label=\"Top Point\")\n",
    "        plt.plot(u_bottom, v_bottom, 'bo', label=\"Bottom Point\")\n",
    "        plt.legend()\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return (u_top, v_top), (u_bottom, v_bottom)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_object_height(depth, segmentation_mask, K, plot, calibrated_depth):\n",
    "    \"\"\"\n",
    "    Compute the object's height using depth information and segmentation results.\n",
    "\n",
    "    Args:\n",
    "        depth: Depth map (in meters).\n",
    "        segmentation_mask: Binary segmentation mask.\n",
    "        K: Camera intrinsic matrix.\n",
    "        plot: Whether to visualize the extracted points.\n",
    "\n",
    "    Returns:\n",
    "        H: Estimated object height (in meters).\n",
    "    \"\"\"\n",
    "    # Extract top and bottom points\n",
    "    (u_top, v_top), (u_bottom, v_bottom) = extract_top_and_bottom_points(segmentation_mask, plot)\n",
    "    depth_u = calibrated_depth[u_top, v_top]\n",
    "    depth_v = calibrated_depth[u_bottom, v_bottom]\n",
    "    print(depth_u, depth_v, depth)\n",
    "    # Compute height in the camera coordinate system\n",
    "    Y_top = pixel_to_camera_coordinates(u_top, v_top, depth, K)[1]\n",
    "    Y_bottom = pixel_to_camera_coordinates(u_bottom, v_bottom, depth, K)[1]\n",
    "    H = abs(Y_top - Y_bottom)\n",
    "\n",
    "    return H\n",
    "\n",
    "\n",
    "def compute_height(depth, path, segmentation_mask, focallength_px=435.19, plot=False):\n",
    "    K = np.array([[ focallength_px, 0, 175],\n",
    "              [0,  focallength_px, 175],\n",
    "              [0,  0,  1]])\n",
    "    \n",
    "    segmentation_mask= np.array(segmentation_mask)\n",
    "    segmentation_mask=segmentation_mask/255\n",
    "    H = compute_object_height(depth, segmentation_mask, K, plot)\n",
    "    print(f\"Recovered object height: {H} meters\")\n",
    "\n",
    "    if plot:\n",
    "        plt.imshow(Image.open(path))\n",
    "        plt.axis('off') \n",
    "        plt.show()\n",
    "    return H\n",
    "\n",
    "def display_images_in_grid(image, images, l ,rows, cols, path,thing_category_ids, figsize=(25, 20)):\n",
    "    \"\"\"\n",
    "    Display images in a grid layout.\n",
    "\n",
    "    Parameters:\n",
    "    - images: list of image arrays (e.g., from plt.imread or PIL.Image)\n",
    "    - rows: number of rows in the grid\n",
    "    - cols: number of columns in the grid\n",
    "    - figsize: size of the figure\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=figsize)\n",
    "    axs = axs.flatten()  # Flatten the 2D array of axes for easy iteration\n",
    "    for i, ax in enumerate(axs):\n",
    "        if i < l:\n",
    "            ax.imshow(image)\n",
    "            ax.imshow(colorize(images==i+1), alpha=0.5)\n",
    "            ax.axis('off')  # Turn off axis for clarity\n",
    "            # print(i)\n",
    "            ax.set_title(list(category_name_to_id.keys())[list(category_name_to_id.values()).index(thing_category_ids[i])])\n",
    "        else:\n",
    "            ax.axis('off')  # Hide empty plots if images < rows*cols\n",
    "            \n",
    "    plt.savefig(path)\n",
    "    # plt.tight_layout()  # Reduce padding between subplots\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def max_area(panoptic_inds, mask, thing_category_ids, thing_category_ids_building, thing_category_ids_):\n",
    "    thing_category_ids=thing_category_ids_+thing_category_ids_building+thing_category_ids\n",
    "    mask=np.array(mask)\n",
    "    mask=mask/255\n",
    "    mask_a = mask==1\n",
    "    max_ind = ''\n",
    "    max_sum = -1\n",
    "    # print(thing_category_ids)\n",
    "    for i in range(len(thing_category_ids)):\n",
    "        m_mask_a = panoptic_inds==i+1\n",
    "        m_mask_a = m_mask_a.numpy()\n",
    "        # plt.imshow(mask)\n",
    "        # plt.imshow(colorize(panoptic_inds==i+1), alpha=0.5)\n",
    "        # plt.show()\n",
    "        overlap = (mask_a & m_mask_a).sum()\n",
    "        tmp = list(category_name_to_id.keys())[list(category_name_to_id.values()).index(thing_category_ids[i])]\n",
    "        # print(tmp, overlap,'max:', max_sum, mask_a.sum())\n",
    "        if overlap>max_sum:\n",
    "            if tmp!=\"building\":\n",
    "                max_sum = overlap\n",
    "                max_ind = tmp\n",
    "    print(\"max_ind:\",max_ind, 'max_sum,',max_sum)\n",
    "    return max_ind\n",
    "import math\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371000  # 地球半径，米\n",
    "    phi1 = math.radians(lat1)\n",
    "    phi2 = math.radians(lat2)\n",
    "    delta_phi = math.radians(lat2 - lat1)\n",
    "    delta_lambda = math.radians(lon2 - lon1)\n",
    "\n",
    "    a = math.sin(delta_phi / 2) ** 2 + math.cos(phi1) * math.cos(phi2) * math.sin(delta_lambda / 2) ** 2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "def compute_camera_to_building_distance(camera_coordinates, building_geometry):\n",
    "    cam_lat, cam_lon = camera_coordinates\n",
    "\n",
    "    # Extract all points of the polygon\n",
    "    coordinates = building_geometry['coordinates'][0]  # outer ring (first list)\n",
    "\n",
    "    # Extract all longitudes and latitudes separately\n",
    "    lons = [point[0] for point in coordinates]\n",
    "    lats = [point[1] for point in coordinates]\n",
    "\n",
    "    # Compute the centroid (simple arithmetic mean)\n",
    "    center_lon = sum(lons) / len(lons)\n",
    "    center_lat = sum(lats) / len(lats)\n",
    "\n",
    "    # Use haversine to compute distance from camera to the building centroid\n",
    "    distance = haversine(cam_lat, cam_lon, center_lat, center_lon)\n",
    "\n",
    "    return distance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54686bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_top_and_bottom_points(segmentation_mask, plot=False):\n",
    "    \"\"\"\n",
    "    Get the top point of every valid column in the mask and a unified bottom\n",
    "    point defined by the mean of the lowest 30% of column bottoms.\n",
    "\n",
    "    - The top point is the highest (smallest v) foreground pixel in each column.\n",
    "    - The bottom point is a single shared v value: compute each column's lowest\n",
    "      foreground pixel, take the lowest 30% of those values, and average them.\n",
    "\n",
    "    Args:\n",
    "        segmentation_mask: Binary mask (building region = 1, background = 0).\n",
    "        plot (bool): Whether to visualize the points (default: False).\n",
    "\n",
    "    Returns:\n",
    "        top_points (List[Tuple[int, int]]): [(u, v_top)] for each valid column.\n",
    "        bottom_points (List[Tuple[int, int]]): [(u, v_bottom_avg)] aligned\n",
    "            one-to-one with top_points.\n",
    "    \"\"\"\n",
    "    height, width = segmentation_mask.shape\n",
    "\n",
    "    top_points = []\n",
    "    bottom_list = []\n",
    "\n",
    "    # Step 1: Collect candidate top and bottom points for all valid columns\n",
    "    for u in range(width):\n",
    "        column = segmentation_mask[:, u]\n",
    "        ys = np.where(column == 1)[0]\n",
    "        if len(ys) > 0:\n",
    "            v_top = ys.min()       # highest foreground pixel in this column\n",
    "            v_bottom = ys.max()    # lowest foreground pixel in this column\n",
    "            top_points.append((u, v_top))\n",
    "            bottom_list.append(v_bottom)\n",
    "\n",
    "    if len(bottom_list) == 0:\n",
    "        raise ValueError(\"No valid region found in the mask.\")\n",
    "\n",
    "    # Step 2: Compute a unified bottom value using the lowest 30% mean\n",
    "    bottom_list_sorted = np.sort(bottom_list)\n",
    "    cutoff = max(int(len(bottom_list_sorted) * 0.3), 1)\n",
    "    v_bottom_avg = int(np.mean(bottom_list_sorted[-cutoff:]))\n",
    "\n",
    "    # Step 3: Build bottom_points (one-to-one with top_points)\n",
    "    bottom_points = [(u, v_bottom_avg) for (u, _) in top_points]\n",
    "\n",
    "    # Step 4: Optional visualization\n",
    "    if plot:\n",
    "        plt.imshow(segmentation_mask, cmap='gray')\n",
    "        for (u, v_top) in top_points:\n",
    "            plt.plot(u, v_top, 'ro', markersize=2)          # top points (red)\n",
    "            plt.plot(u, v_bottom_avg, 'bo', markersize=2)   # unified bottom (blue)\n",
    "        plt.plot([0, width - 1], [v_bottom_avg, v_bottom_avg],\n",
    "                 'b--', label='Bottom 30% Mean')\n",
    "        plt.title(f\"Column Heights: (bottom avg) {v_bottom_avg} - v_top[i]\")\n",
    "        plt.legend()\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return top_points, bottom_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6701238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image  # needed if plot=True in compute_height\n",
    "\n",
    "def cluster_and_get_largest_mean(data, threshold=1.0):\n",
    "    \"\"\"\n",
    "    Cluster 1D values by grouping consecutive numbers whose pairwise\n",
    "    differences are <= `threshold`, and return the mean of the largest cluster.\n",
    "    \"\"\"\n",
    "    data = sorted(data)\n",
    "    clusters = []\n",
    "    current_cluster = [data[0]]\n",
    "    \n",
    "    for i in range(1, len(data)):\n",
    "        if abs(data[i] - current_cluster[-1]) <= threshold:\n",
    "            current_cluster.append(data[i])\n",
    "        else:\n",
    "            clusters.append(current_cluster)\n",
    "            current_cluster = [data[i]]\n",
    "    clusters.append(current_cluster)  # include the last cluster\n",
    "    \n",
    "    # Pick the cluster with the most samples\n",
    "    max_cluster = max(clusters, key=len)\n",
    "    return np.mean(max_cluster)\n",
    "\n",
    "\n",
    "def decide_distance(segmentation_mask, depth_image):\n",
    "    \"\"\"\n",
    "    Within the masked region, collect depth values and estimate the distance by\n",
    "    clustering and averaging the largest cluster.\n",
    "\n",
    "    Args:\n",
    "        segmentation_mask (np.ndarray): Binary mask with shape (H, W). Target = 1, background = 0.\n",
    "        depth_image (np.ndarray): Depth map with shape (H, W).\n",
    "\n",
    "    Returns:\n",
    "        float: Estimated distance. Returns np.nan if no valid values exist.\n",
    "    \"\"\"\n",
    "    valid_depths = depth_image[segmentation_mask > 0]\n",
    "    # If the depth map is normalized, you could invert it like this:\n",
    "    # valid_depths = 1 - valid_depths / 255.0  # assuming normalized depth; 1 means farther\n",
    "\n",
    "    # Remove invalid/anomalous values (zeros or NaNs)\n",
    "    valid_depths = valid_depths[np.isfinite(valid_depths)]\n",
    "    valid_depths = valid_depths[valid_depths > 0]\n",
    "\n",
    "    if len(valid_depths) == 0:\n",
    "        return np.nan\n",
    "\n",
    "    # Cluster the sorted depths and return the mean of the largest cluster\n",
    "    sorted_depths = np.sort(valid_depths)\n",
    "    return cluster_and_get_largest_mean(sorted_depths, threshold=0.1)\n",
    "\n",
    "    # Alternative: trim the lowest and highest 30% and average the middle 40%\n",
    "    # n = len(sorted_depths)\n",
    "    # lower = int(n * 0.3)\n",
    "    # upper = int(n * 0.7)\n",
    "    # trimmed = sorted_depths[lower:upper]\n",
    "    # if len(trimmed) == 0:\n",
    "    #     return np.nan\n",
    "    # return np.mean(trimmed)\n",
    " \n",
    "\n",
    "# def calibrate_and_filter_mask(segmentation_mask, depth_image, distance, tolerance=2):\n",
    "#     \"\"\"\n",
    "#     Calibrate `depth_image` using the center pixel and create a new mask that\n",
    "#     keeps only pixels within [distance ± tolerance].\n",
    "#\n",
    "#     Args:\n",
    "#         segmentation_mask (ndarray): Binary mask of shape [H, W].\n",
    "#         depth_image (ndarray): Uncalibrated normalized depth image in [0, 1].\n",
    "#         distance (float): Ground-truth depth (meters) at the image center.\n",
    "#         tolerance (float): Allowed absolute error (meters), default ±0.5 m.\n",
    "#\n",
    "#     Returns:\n",
    "#         new_mask (ndarray): Binary mask of the same shape; 1 if within range.\n",
    "#         depth_in_meters (ndarray): Calibrated depth map in meters.\n",
    "#     \"\"\"\n",
    "#     depth_image = 1 - np.array(depth_image).astype(np.float32) / 255\n",
    "#     height, width = segmentation_mask.shape\n",
    "#     cy, cx = height // 2, width // 2\n",
    "#\n",
    "#     center_depth_value = depth_image[cy, cx]\n",
    "#     if center_depth_value == 0:\n",
    "#         raise ValueError(\"Center depth is 0; likely an invalid region.\")\n",
    "#\n",
    "#     # Calibration: real_depth = normalized_depth * scale\n",
    "#     scale = distance / center_depth_value\n",
    "#     depth_in_meters = depth_image * scale\n",
    "#     new_distance = decide_distance(segmentation_mask, depth_in_meters)\n",
    "#     print(new_distance, distance)\n",
    "#\n",
    "#     # Build a new mask for pixels within [new_distance ± tolerance]\n",
    "#     within_range = ((depth_in_meters >= (new_distance - tolerance)) &\n",
    "#                     (depth_in_meters <= (new_distance + tolerance)))\n",
    "#     new_mask = (segmentation_mask > 0) & within_range\n",
    "#     return new_mask.astype(np.uint8), depth_in_meters, new_distance\n",
    "\n",
    "\n",
    "def calibrate_and_filter_mask(left_point, right_point, segmentation_mask, depth_image, distance, tolerance=2.0):\n",
    "    \"\"\"\n",
    "    Fit a linear transformation using left, right, and center anchor points,\n",
    "    calibrate the depth map, and then filter pixels by a depth band.\n",
    "\n",
    "    Args:\n",
    "        left_point (dict): Contains 'x_pixel' and 'distance_m'.\n",
    "        right_point (dict): Same keys as left_point.\n",
    "        segmentation_mask (np.ndarray): Binary mask, shape [H, W].\n",
    "        depth_image (np.ndarray): Original depth map in millimeters.\n",
    "        distance (float): Ground-truth depth (meters) at the image center.\n",
    "        tolerance (float): Allowed absolute error (meters), default ±2.0 m.\n",
    "\n",
    "    Returns:\n",
    "        new_mask (np.ndarray): Binary mask (0/1) after depth filtering.\n",
    "        depth_in_meters (np.ndarray): Calibrated depth map.\n",
    "        new_distance (float): Median depth within the (masked) region.\n",
    "    \"\"\"\n",
    "    depth_image = np.array(depth_image).astype(np.float32) / 1000.0  # convert mm to meters\n",
    "    height, width = segmentation_mask.shape\n",
    "    cy, cx = height // 2, width // 2\n",
    "\n",
    "    # Extract center depth\n",
    "    center_depth = depth_image[cy, cx]\n",
    "    if center_depth == 0:\n",
    "        raise ValueError(\"Center depth is 0; likely an invalid region.\")\n",
    "\n",
    "    # Build anchor pairs: (raw_depth_pixel, ground_truth_distance_m)\n",
    "    points = [\n",
    "        (depth_image[cy, left_point['x_pixel']], left_point['distance_m']),\n",
    "        (depth_image[cy, right_point['x_pixel']], right_point['distance_m']),\n",
    "        (center_depth, distance),\n",
    "    ]\n",
    "\n",
    "    # Fit A, B s.t. distance_m ≈ A * depth_pixel + B (least squares)\n",
    "    A_matrix = np.array([[d, 1] for d, _ in points])   # [[d1, 1], [d2, 1], ...]\n",
    "    b_vector = np.array([m for _, m in points])         # [m1, m2, m3]\n",
    "    A, B = np.linalg.lstsq(A_matrix, b_vector, rcond=None)[0]\n",
    "\n",
    "    # Apply linear calibration\n",
    "    depth_in_meters = A * depth_image + B\n",
    "\n",
    "    # Compute median depth within the masked region (ignore zeros)\n",
    "    masked_depth = depth_in_meters[segmentation_mask > 0]\n",
    "    masked_depth = masked_depth[masked_depth > 0]\n",
    "    new_distance = np.median(masked_depth) if masked_depth.size > 0 else np.nan\n",
    "\n",
    "    # Build a new mask for pixels within [new_distance ± tolerance]\n",
    "    within_range = ((depth_in_meters >= new_distance - tolerance) &\n",
    "                    (depth_in_meters <= new_distance + tolerance))\n",
    "    new_mask = (segmentation_mask > 0) & within_range\n",
    "\n",
    "    return new_mask.astype(np.uint8), depth_in_meters, new_distance\n",
    "\n",
    "\n",
    "def compute_height(left_point, right_point, depth, path, segmentation_mask, depth_image,\n",
    "                   focallength_px=435.19, plot=False):\n",
    "    \"\"\"\n",
    "    Calibrate the depth map using three anchor points, filter the mask by the\n",
    "    calibrated depth band, and compute object height (via `compute_object_height`).\n",
    "\n",
    "    Args:\n",
    "        left_point (dict): Contains 'x_pixel' and 'distance_m'.\n",
    "        right_point (dict): Contains 'x_pixel' and 'distance_m'.\n",
    "        depth (float): Ground-truth center distance (meters).\n",
    "        path (str): Image file path for optional visualization.\n",
    "        segmentation_mask (np.ndarray): Binary mask in [0, 255]; will be scaled to {0,1}.\n",
    "        depth_image (np.ndarray): Depth map in millimeters.\n",
    "        focallength_px (float): Focal length in pixels (for K).\n",
    "        plot (bool): If True, show the original image.\n",
    "\n",
    "    Returns:\n",
    "        float: Estimated object height in meters.\n",
    "    \"\"\"\n",
    "    K = np.array([[focallength_px, 0, 175],\n",
    "                  [0, focallength_px, 175],\n",
    "                  [0, 0, 1]])\n",
    "    \n",
    "    segmentation_mask = np.array(segmentation_mask).astype(np.float32) / 255.0\n",
    "    new_mask, calibrated_depth, new_distance = calibrate_and_filter_mask(\n",
    "        left_point, right_point, segmentation_mask, depth_image, depth\n",
    "    )\n",
    "\n",
    "    # Note: `compute_object_height` must be defined elsewhere.\n",
    "    H = compute_object_height(depth, new_distance, new_mask, K, plot, calibrated_depth)\n",
    "    print(f\"Recovered object height: {H} meters\")\n",
    "\n",
    "    if plot:\n",
    "        plt.imshow(Image.open(path))\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685c6d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_object_height(depth, new_depth, segmentation_mask, K, plot, calibrated_depth):\n",
    "    \"\"\"\n",
    "    Compute object heights using multiple top and bottom points.\n",
    "\n",
    "    Args:\n",
    "        depth: Depth map (in meters), shape (H, W).\n",
    "        new_depth: [Unused] Reserved for future input.\n",
    "        segmentation_mask: Binary mask, shape (H, W), object = 1.\n",
    "        K: Camera intrinsic matrix, shape (3, 3).\n",
    "        plot: Whether to visualize the top/bottom point locations.\n",
    "        calibrated_depth: Depth map for extracting accurate depth at top/bottom points.\n",
    "\n",
    "    Returns:\n",
    "        heights: List of object height estimations (in meters), one for each top-bottom pair.\n",
    "    \"\"\"\n",
    "    # Extract all valid top and corresponding bottom points\n",
    "    top_points, bottom_points = extract_top_and_bottom_points(segmentation_mask, plot)\n",
    "    # print(len(top_points), len(bottom_points))\n",
    "    heights = []\n",
    "    for (u_top, v_top), (u_bottom, v_bottom) in zip(top_points, bottom_points):\n",
    "        # 深度值提取（避免无效点）\n",
    "        if not (0 <= v_top < calibrated_depth.shape[0] and 0 <= u_top < calibrated_depth.shape[1]):\n",
    "            continue\n",
    "        if not (0 <= v_bottom < calibrated_depth.shape[0] and 0 <= u_bottom < calibrated_depth.shape[1]):\n",
    "            continue\n",
    "        depth_top = calibrated_depth[v_top, u_top]\n",
    "        depth_bottom = calibrated_depth[v_bottom, u_bottom]\n",
    "        if abs(depth_top-depth)>3:\n",
    "            depth_top = depth\n",
    "        if abs(depth_bottom-depth)>3:\n",
    "            depth_bottom = depth\n",
    "        # 跳过深度无效点（0或nan）\n",
    "        if np.isnan(depth_top) or np.isnan(depth_bottom) or depth_top <= 0 or depth_bottom <= 0:\n",
    "            continue\n",
    "        # 计算相机坐标系下的Y值（高度）\n",
    "        Y_top = pixel_to_camera_coordinates(u_top, v_top, depth_top, K)[1]\n",
    "        Y_bottom = pixel_to_camera_coordinates(u_bottom, v_bottom, depth_bottom, K)[1]\n",
    "\n",
    "        height = abs(Y_bottom - Y_top)\n",
    "        heights.append(height)\n",
    "    return sorted(heights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aac32d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Custom library imports (make sure you have these functions and models)\n",
    "# from your_module import BuildingDataset, haversine, generate_buiding_panoptic_mask, compute_height, \\\n",
    "#     max_area, display_images_in_grid, colorize\n",
    "\n",
    "# File paths\n",
    "image_root = r\"F:\\Projects\\msc_personal_project\\GoogleStreetScript\"\n",
    "json_path = r\"F:\\Projects\\msc_personal_project\\GoogleStreetScript\\data\\osm\\processed_buildings_with_projections84.json\"\n",
    "json_save_path = Path(r\"F:\\Projects\\msc_personal_project\\result\\0516\\building_heights_v0810_view.json\")\n",
    "depth_root = r\"F:\\Projects\\msc_personal_project\\GoogleStreetScript\\data\\depth_maps\"\n",
    "\n",
    "# Load dataset\n",
    "dataset = BuildingDataset(json_path)\n",
    "\n",
    "# Buffer for JSON results\n",
    "results_data = []\n",
    "\n",
    "# Iterate over the dataset\n",
    "for i in range(len(dataset)):\n",
    "    try:\n",
    "        metadata, image = dataset[i]\n",
    "        image_path = os.path.join(image_root, metadata[\"fixed_pitch_image\"])\n",
    "\n",
    "        # Distance from camera location to building (in meters)\n",
    "        distance = metadata['intersection_point_projection']['distance_m']\n",
    "\n",
    "        # Generate building panoptic mask\n",
    "        # panoptic_inds, thing_category_ids, panoptic_inds_buildings, building_masks = generate_buiding_panoptic_mask(\n",
    "        #     image, thing_category_names, stuff_category_names_building,\n",
    "        #     stuff_category_names, category_name_to_id,\n",
    "        #     groundingdino_model, sam_predictor,\n",
    "        #     clipseg_processor, clipseg_model, device\n",
    "        # )\n",
    "\n",
    "        # Binarize mask\n",
    "        # threshold = 0.5\n",
    "        # binary_masks = [(mask >= threshold).to(torch.uint8) for mask in building_masks]\n",
    "\n",
    "        left_point = metadata['left_point_projection']\n",
    "        right_point = metadata['right_point_projection']\n",
    "        building_entries = []\n",
    "\n",
    "        # thing_category_ids_ = [category_name_to_id[name] for name in stuff_category_names]\n",
    "        # thing_category_ids_building = [category_name_to_id[name] for name in stuff_category_names_building]\n",
    "\n",
    "        for k in range(5):\n",
    "            mask_path = f'F:/Projects/msc_personal_project/result/0516/{i}_{k}_building_mask.png'\n",
    "            if os.path.exists(mask_path):\n",
    "                binary_image = Image.open(mask_path).convert(\"L\")\n",
    "                # binary_image = Image.fromarray(binary_mask.squeeze().cpu().numpy() * 255).convert(\"L\")\n",
    "\n",
    "                depth_path = os.path.join(depth_root, f'{metadata[\"osm_id\"]}.png')\n",
    "                depth_image = Image.open(depth_path)\n",
    "\n",
    "                # Compute category index (if needed)\n",
    "                # max_ind = max_area(\n",
    "                #     panoptic_inds, binary_image,\n",
    "                #     thing_category_ids, thing_category_ids_building, thing_category_ids_\n",
    "                # )\n",
    "\n",
    "                # Estimate building height\n",
    "                # (435.19 is focal length; here we pass 400 as an example focal length)\n",
    "                H = compute_height(left_point, right_point, distance, image_path, binary_image, depth_image, 400, False)\n",
    "                # H = compute_height(distance, image_path, binary_image, 250, True)\n",
    "\n",
    "                print(metadata[\"new_height\"])\n",
    "                print(f\"Building height: {H[-1]} meters\")\n",
    "\n",
    "                building_entries.append({\n",
    "                    \"H\": H,\n",
    "                    # \"max_ind\": max_ind,\n",
    "                    \"building_mask_path\": mask_path\n",
    "                })\n",
    "\n",
    "        # # Show and save mask grid\n",
    "        # all_ids = thing_category_ids_ + thing_category_ids_building + thing_category_ids\n",
    "        # display_images_in_grid(\n",
    "        #     image,\n",
    "        #     panoptic_inds,\n",
    "        #     len(all_ids),\n",
    "        #     rows=8,\n",
    "        #     cols=6,\n",
    "        #     path=f'F:/Projects/msc_personal_project/result/0516/{i}_all.png',\n",
    "        #     thing_category_ids=all_ids\n",
    "        # )\n",
    "\n",
    "        # # Save overlay visualization\n",
    "        # _, ax = plt.subplots()\n",
    "        # ax.imshow(image)\n",
    "        # ax.imshow(colorize(panoptic_inds), alpha=0.5)\n",
    "        # plt.savefig(f'F:/Projects/msc_personal_project/result/0516/{i}_seg.png', bbox_inches='tight', dpi=300)\n",
    "        # plt.close()\n",
    "\n",
    "        # Record results\n",
    "        result_entry = {\n",
    "            \"idx\": i,\n",
    "            \"image_path\": image_path,\n",
    "            \"buildings\": building_entries,\n",
    "            \"original_height\": metadata[\"new_height\"]\n",
    "        }\n",
    "        results_data.append(result_entry)\n",
    "\n",
    "        # Incrementally write to JSON\n",
    "        with open(json_save_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "            json.dump(results_data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "        print(f\"[{i}] Done.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[{i}] Error occurred: {e}\")\n",
    "\n",
    "print(f\"All results saved to {json_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16daa3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Custom library imports (make sure you have these functions and models)\n",
    "# from your_module import BuildingDataset, haversine, generate_buiding_panoptic_mask, compute_height, \\\n",
    "#     max_area, display_images_in_grid, colorize\n",
    "\n",
    "# File paths\n",
    "image_root = r\"F:\\Projects\\msc_personal_project\\GoogleStreetScript\"\n",
    "json_path = r\"F:\\Projects\\msc_personal_project\\GoogleStreetScript\\data\\osm\\filtered_buildings_heading_distance_height.json\"\n",
    "json_save_path = Path(r\"F:\\Projects\\msc_personal_project\\result\\0516\\building_heights_v0802.json\")\n",
    "depth_root = r\"F:\\Projects\\msc_personal_project\\GoogleStreetScript\\data\\depth_maps\"\n",
    "\n",
    "# Load dataset\n",
    "dataset = BuildingDataset(json_path)\n",
    "\n",
    "# Results buffer for JSON\n",
    "results_data = []\n",
    "\n",
    "# Iterate over the dataset\n",
    "for i in range(len(dataset)):\n",
    "    try:\n",
    "        metadata, image = dataset[i]\n",
    "        image_path = os.path.join(image_root, metadata[\"fixed_pitch_image\"])\n",
    "\n",
    "        # Distance from camera to building (meters)\n",
    "        distance = metadata['camera_to_building_distance_m']\n",
    "\n",
    "        # Generate building panoptic mask\n",
    "        # panoptic_inds, thing_category_ids, panoptic_inds_buildings, building_masks = generate_buiding_panoptic_mask(\n",
    "        #     image, thing_category_names, stuff_category_names_building,\n",
    "        #     stuff_category_names, category_name_to_id,\n",
    "        #     groundingdino_model, sam_predictor,\n",
    "        #     clipseg_processor, clipseg_model, device\n",
    "        # )\n",
    "\n",
    "        # Binarize masks\n",
    "        # threshold = 0.5\n",
    "        # binary_masks = [(mask >= threshold).to(torch.uint8) for mask in building_masks]\n",
    "\n",
    "        building_entries = []\n",
    "        thing_category_ids_ = [category_name_to_id[name] for name in stuff_category_names]\n",
    "        thing_category_ids_building = [category_name_to_id[name] for name in stuff_category_names_building]\n",
    "\n",
    "        for k in range(5):\n",
    "            mask_path = f'F:/Projects/msc_personal_project/result/0516/{i}_{k}_building_mask.png'\n",
    "            if os.path.exists(mask_path):\n",
    "                binary_image = Image.open(mask_path).convert(\"L\")\n",
    "                # binary_image = Image.fromarray(binary_mask.squeeze().cpu().numpy() * 255).convert(\"L\")\n",
    "\n",
    "                depth_path = os.path.join(depth_root, f'{metadata[\"osm_id\"]}.png')\n",
    "                depth_image = Image.open(depth_path)\n",
    "\n",
    "                # Compute category index (if needed)\n",
    "                # max_ind = max_area(\n",
    "                #     panoptic_inds, binary_image,\n",
    "                #     thing_category_ids, thing_category_ids_building, thing_category_ids_\n",
    "                # )\n",
    "\n",
    "                # Compute building height\n",
    "                # (435.19 is a typical focal length; here 250 is provided as an example)\n",
    "                H = compute_height(distance, image_path, binary_image, depth_image, 250, False)\n",
    "                # H = compute_height(distance, image_path, binary_image, 250, True)\n",
    "\n",
    "                print(f\"Building height: {H[-1]} meters, original height: {metadata['new_height']}\")\n",
    "                building_entries.append({\n",
    "                    \"H\": H,\n",
    "                    # \"max_ind\": max_ind,\n",
    "                    \"building_mask_path\": mask_path\n",
    "                })\n",
    "\n",
    "        # # Show and save mask grid\n",
    "        # all_ids = thing_category_ids_ + thing_category_ids_building + thing_category_ids\n",
    "        # display_images_in_grid(\n",
    "        #     image,\n",
    "        #     panoptic_inds,\n",
    "        #     len(all_ids),\n",
    "        #     rows=8,\n",
    "        #     cols=6,\n",
    "        #     path=f'F:/Projects/msc_personal_project/result/0516/{i}_all.png',\n",
    "        #     thing_category_ids=all_ids\n",
    "        # )\n",
    "\n",
    "        # # Save overlay visualization\n",
    "        # _, ax = plt.subplots()\n",
    "        # ax.imshow(image)\n",
    "        # ax.imshow(colorize(panoptic_inds), alpha=0.5)\n",
    "        # plt.savefig(f'F:/Projects/msc_personal_project/result/0516/{i}_seg.png', bbox_inches='tight', dpi=300)\n",
    "        # plt.close()\n",
    "\n",
    "        # Record this sample’s result\n",
    "        result_entry = {\n",
    "            \"idx\": i,\n",
    "            \"image_path\": image_path,\n",
    "            \"buildings\": building_entries,\n",
    "            \"original_height\": metadata[\"new_height\"]\n",
    "        }\n",
    "        results_data.append(result_entry)\n",
    "\n",
    "        # Incrementally write to JSON\n",
    "        with open(json_save_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "            json.dump(results_data, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "        print(f\"[{i}] Done.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[{i}] Error occurred: {e}\")\n",
    "\n",
    "print(f\"All results have been saved to {json_save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cameractrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
